{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240c1868-61a1-457d-b23f-3b55d9dcf8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import data\n",
    "use python to downlaod and load statscanada odhf dataset into catalogs, schema and unique table specified below \n",
    "\n",
    "\n",
    "make sur eyou change catalog and schema name to workshop specific catalog and schema and make table name unique tp your self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3dd614-4666-4017-a8c1-49bd616d6ef7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define variables"
    }
   },
   "outputs": [],
   "source": [
    "# Define variables for catalog name, schema, and table name\n",
    "catalog_name = \"razi_demo\"\n",
    "schema_name = \"vch_workshop\"\n",
    "table_name = \"raw_odhf_razi_data\"\n",
    "cleaned_table_name = \"clean_odhf_razi_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea5711c-c9a9-4f77-94bb-55f71182a06e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "download and load"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Download and Load statscanada odhf dataset\n",
    "\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://www150.statcan.gc.ca/n1/en/pub/13-26-0001/2020001/ODHF_v1.1.zip\"\n",
    "response = requests.get(url)\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    with z.open('ODHF_v1.1/odhf_v1.1.csv') as f:\n",
    "        raw_df = pd.read_csv(f,encoding='ISO-8859-1')\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff798a62-04d6-48db-9b8d-de5a11ce9338",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save df as table in UC"
    }
   },
   "outputs": [],
   "source": [
    "#step 2:  convert df to a Spark DataFrame first and then write the Spark DataFrame to a table\n",
    " \n",
    "spark_df = spark.createDataFrame(raw_df)\n",
    "\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee8e87b-5921-4274-bece-3e12b6d1b452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data enrichment\n",
    "\n",
    "using teh data profile here we realized 6.88% of rows miss lattitude and longitude, let's drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8eb151b-885e-4fba-ac94-b1b75ef208dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clean data"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = raw_df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "display(df_cleaned)\n",
    "# use data profile to confirm the lat/long are valid now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04013f4-87cd-469c-81b5-768704eb02b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save clean data to UC"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned_spark = spark.createDataFrame(df_cleaned)\n",
    "df_cleaned_spark.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{cleaned_table_name}\")\n",
    "\n",
    "display(df_cleaned_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb669e6-e906-4d65-94c0-68197bf68574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create visualization\n",
    "\n",
    "visualize the facilities on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36975032-edea-4f3a-8716-ca15a21da956",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install folium\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2028a5d7-08e0-4a8b-baf2-a1284bb43834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure df_cleaned is a Spark DataFrame\n",
    "df_cleaned = spark.createDataFrame(df_cleaned)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_pandas = df_cleaned.toPandas()\n",
    "\n",
    "# Create a map centered around the average latitude and longitude\n",
    "map_center = [df_pandas['latitude'].mean(), df_pandas['longitude'].mean()]\n",
    "facility_map = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add a marker cluster to the map\n",
    "marker_cluster = MarkerCluster().add_to(facility_map)\n",
    "\n",
    "# Add markers to the cluster\n",
    "for idx, row in df_pandas.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['facility_name']\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Display the map\n",
    "display(facility_map)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "odhf data without volumes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
