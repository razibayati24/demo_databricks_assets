{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c97f095-e30f-4eeb-be40-9bacd2769466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prerequisite\n",
    "\n",
    "- go to your target catalog and schema,\n",
    "- click on volumes\n",
    "- click on create drop down\n",
    "- click on \"create Volume\"\n",
    "\n",
    "\n",
    "![Screenshot 1.png](./Screenshot 1.png \"Screenshot 1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dbe4901-9b27-41c1-91b0-977ee2fa12ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "call it a unique name: \"your_name_directory\" \n",
    "\n",
    "![Screenshot 2025-03-11 at 11.52.39 AM.png](./Screenshot 2025-03-11 at 11.52.39 AM.png \"Screenshot 2025-03-11 at 11.52.39 AM.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbad209d-5d4f-4504-9613-918301de360b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "now you'd be able to see your directory under volumes:\n",
    "\n",
    "![Screenshot 2025-03-11 at 11.55.28 AM.png](./Screenshot 2025-03-11 at 11.55.28 AM.png \"Screenshot 2025-03-11 at 11.55.28 AM.png\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bddaeef5-8626-4899-9617-e33036b74a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import data\n",
    "you can do this section manually  \n",
    "- download teh zip file from https://www150.statcan.gc.ca/n1/en/pub/13-26-0001/2020001/ODHF_v1.1.zip \n",
    "- unzip on your local computer \n",
    "- upload the csv file it to directory you created\n",
    "- Go to the csv file in volumes, create table and specity catalog and schema and table name\n",
    "\n",
    "or \n",
    "\n",
    "use python to download and unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbc2adf-4d1f-40bf-a3c2-adf54cf0a234",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "set your directory path"
    }
   },
   "outputs": [],
   "source": [
    "directory = '/Volumes/razi_demo/vch_workshop/razi_bayati_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5e3f2e-dc66-4257-be3f-204c473625aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "download zip file into directory"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://www150.statcan.gc.ca/n1/en/pub/13-26-0001/2020001/ODHF_v1.1.zip\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "file_path = os.path.join(directory, \"ODHF_v1.1.zip\")\n",
    "with open(file_path, \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf393098-fe22-4aac-8834-d42c07387ffb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "unzip the file"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7eb908-45cd-4bf5-b512-44b8194f36df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create spark df"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBSZWFkIHRoZSBDU1YgZmlsZSBpbnRvIGEgU3BhcmsgRGF0YUZyYW1lLCBjcmVhdGUgYSB0ZW1wb3JhcnkgdmlldywgYW5kIGRpc3BsYXkgdGhlIERhdGFGcmFtZQpjc3ZfZmlsZV9wYXRoID0gb3MucGF0aC5qb2luKGRpcmVjdG9yeSwgIk9ESEZfdjEuMSIsICJvZGhmX3YxLjEuY3N2IikKZGYgPSBzcGFyay5yZWFkLmNzdihjc3ZfZmlsZV9wYXRoLCBoZWFkZXI9VHJ1ZSwgaW5mZXJTY2hlbWE9VHJ1ZSkKZGYuY3JlYXRlT3JSZXBsYWNlVGVtcFZpZXcoIm9kaGZfdjFfMSIpCmRpc3BsYXkoZGYpCiMgZXhwbG9yZSBkYXRhIHByb2ZpbGUgdXNpbmcgdGhlICsgYmVzaWRlIHRoZSB0YWJsZSBpY29u\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1741720695078,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "7c18f5c1-8611-4546-a243-c827eeb537c7",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1741720689718,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1741720689622,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the CSV file into a Spark DataFrame, create a temporary view, and display the DataFrame\n",
    "csv_file_path = os.path.join(directory, \"ODHF_v1.1\", \"odhf_v1.1.csv\")\n",
    "raw_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "raw_df.createOrReplaceTempView(\"odhf_v1_1\")\n",
    "display(raw_df)\n",
    "# explore data profile using the + beside the table icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03391cc9-d055-4b0c-84f3-a359e1619519",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save it as a table to UC"
    }
   },
   "outputs": [],
   "source": [
    "raw_df.write.mode(\"overwrite\").saveAsTable(\"razi_demo.vch_workshop.raw_odhf_using_python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da92267-213e-425b-a6ec-ec3bfff5e111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "now you can go back to UC and start exploring the data there "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9692fb14-2e79-48e5-a025-d0695c12fbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data enrichment\n",
    "\n",
    "using teh data profile here we realized 6.88% of rows miss lattitude and longitude, let's drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74576fdb-7d1c-45c6-8240-5828136ac2f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "drop rows with missing attitude and longitude"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGZfY2xlYW5lZCA9IHJhd19kZi5kcm9wbmEoc3Vic2V0PVsibGF0aXR1ZGUiLCAibG9uZ2l0dWRlIl0pCmRpc3BsYXkoZGZfY2xlYW5lZCk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1741720921297,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "4d2de0be-6652-490c-bc56-b82b35852bc7",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1741720919519,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1741720919442,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned = raw_df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "display(df_cleaned)\n",
    "# use data profile to confirm the lat/long are valid now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df704daa-52f3-41da-8eeb-e03ccb0e65d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save as a cleaned table on UC"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned.write.mode(\"overwrite\").saveAsTable(\"razi_demo.vch_workshop.cleaned_odhf_using_python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78a92df6-5d20-44ac-b508-3f055b895d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "go to UC and look at lineage tab for the new table\n",
    "\n",
    "![Screenshot 2025-03-11 at 12.30.48 PM.png](./Screenshot 2025-03-11 at 12.30.48 PM.png \"Screenshot 2025-03-11 at 12.30.48 PM.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c4a669-5100-43e5-a277-a2cd97e42ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create visualization\n",
    "\n",
    "visualize the facilities on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0de0e5-6171-47b6-9a2c-984678bbb64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install folium\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637bb7ff-4301-488b-bc27-fc2089ffcccc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Spark DataFrame to Pandas DataFrame and visualize the facilities on map"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_pandas = df_cleaned.toPandas()\n",
    "\n",
    "# Create a map centered around the average latitude and longitude\n",
    "map_center = [df_pandas['latitude'].mean(), df_pandas['longitude'].mean()]\n",
    "facility_map = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add a marker cluster to the map\n",
    "marker_cluster = MarkerCluster().add_to(facility_map)\n",
    "\n",
    "# Add markers to the cluster\n",
    "for idx, row in df_pandas.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['facility_name']\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Display the map\n",
    "display(facility_map)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Demo - odhf data with volumes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
